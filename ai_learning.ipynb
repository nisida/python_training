{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e443a2db",
   "metadata": {},
   "source": [
    "# 目標\n",
    "\n",
    "## 機械学習の基礎を理解して、独自で基本的な学習モデルのコードを実装できるようになる。\n",
    "\n",
    "# 学習内容\n",
    "\n",
    "## - AIとは何か\n",
    "## - 機械学習の基本原理とは\n",
    "## - 機械学習を構成する要素\n",
    "## - 機械学習モデルとは\n",
    "## - TensorFlowとPythonによるautoencoderの実装\n",
    "## - 画像分類技術の基礎\n",
    "## - TensorFlowとPythonによる画像分類の実装\n",
    "## - スパムメール分類\n",
    "## - 強化学習の基礎\n",
    "## - 迷路探索アルゴリズムの実装(各自)\n",
    "## - 生成AIの使い方（プロンプト）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43035df2",
   "metadata": {},
   "source": [
    "## 単純パーセプトロン　\n",
    "\n",
    "### 入門\n",
    "\n",
    "#### パーセプトロンの定義：生物学的着想から人工ニューロンへ\n",
    "\n",
    "パーセプトロンは、ニューラルネットワークの分野における最も基本的なアルゴリズムであり、その起源は生物の神経細胞（ニューロン）の信号処理メカニズムにあります 。ニューロンは、他のニューロンから信号を受け取り（樹状突起）、それを細胞体で処理し、信号の強さが特定の閾値を超えると次のニューロンへ信号を出力（軸索発火）します 。パーセプトロンは、この生物学的なプロセスを数学的に模倣したモデルです 。   \n",
    "\n",
    "具体的には、「人工ニューロン」とも呼ばれる単純パーセプトロンは、複数の二値（0か1）の入力を受け取り、それらに基づいて単一の二値（0か1）の出力を生成する非常にシンプルな計算ユニットです 。これはあくまで生物学的ニューロンの機能を大幅に簡略化したモデルである点に注意が必要です 。   \n",
    "\n",
    "#### 歴史的背景と意義（フランク・ローゼンブラット）\n",
    "\n",
    "パーセプトロンは、1957年から1958年にかけて、アメリカの心理学者フランク・ローゼンブラットによって考案されました 。これはニューラルネットワーク研究の原点と見なされており 、後のディープラーニング技術の先駆けとなる重要な概念でした 。発表当初は大きな期待を集めましたが、後にマービン・ミンスキーとシーモア・パパートによってその限界（特にXOR問題のような線形分離不可能な問題を解けないこと）が指摘され、これが第一次AIブームの終焉、いわゆる「AIの冬」の一因となったとも言われています 。   \n",
    "\n",
    "#### ニューラルネットワークの礎石としてのパーセプトロン\n",
    "\n",
    "「単純パーセプトロン」とは、通常、単一の計算ユニットを指します。これは、より複雑なネットワーク構造を理解するための基本的な構成要素（ビルディングブロック）です 。単純パーセプトロンの仕組みを理解することは、多層パーセプトロン（MLP）やディープラーニングといった、より高度なモデルを学ぶ上で不可欠です 。   \n",
    "\n",
    "なお、「パーセプトロン」という用語は、文脈によってこの単純なモデル（単層）を指す場合と、複数のパーセプトロンを組み合わせた多層構造を指す場合と、複数のパーセプトロンを組み合わせた多層構造を指す場合があります 。本レポートでは、主に「単純パーセプトロン」に焦点を当てて解説を進めます。   \n",
    "\n",
    "### 構造\n",
    "\n",
    "#### アーキテクチャ概要：入力層と出力層\n",
    "\n",
    "単純パーセプトロンは、非常に基本的な二層構造を持っています。信号を受け取る「入力層」と、単一の結果を出力する「出力層」のみで構成されます 。多層パーセプトロン（MLP）との大きな違いは、入力層と出力層の間に「中間層（隠れ層）」が存在しない点です 。   \n",
    "\n",
    "![Perceptron](./perceptron.png)\n",
    "\n",
    "#### 主要な構成要素\n",
    "\n",
    "単純パーセプトロンは、以下の要素から成り立っています。\n",
    "\n",
    "- 入力 (Input, 特徴量, x): パーセプトロンに与えられるデータの特徴を表す信号です。通常、複数の入力 $(x_1,x_2, ..... , x_n)$を受け取ります 。   \n",
    "- 重み (Weight, シナプス強度, w): 各入力信号 $x_i$ に関連付けられた数値であり、その入力が出力に与える影響の大きさ、すなわち重要度を示します 。どの入力がより重要かを制御するパラメータであり $(w_1, w_2, ..... , w_n)$ 、学習プロセスを通じてこれらの重みが調整されます 。   \n",
    "- バイアス (Bias, b) / 閾値 (Threshold, θ):\n",
    "   - 閾値 (θ): パーセプトロンが「発火」（出力を1とする）するために、入力の重み付き和が超えるべき境界値です 。ニューロンの発火のしやすさを制御します 。   \n",
    "   - バイアス (b): 閾値と同様の役割を果たすパラメータで、数式上は b=−θ の関係にあります 。バイアスは、決定境界の位置を平行移動させる効果を持ち、活性化の閾値を調整します 。実装上は、常に+1の値を持つ追加の入力 $x_0$に対応する重み$w_0$として扱われることがよくあります 。   \n",
    "- 活性化関数 (Activation Function, ステップ関数): 最終的な出力を決定する関数です。単純パーセプトロンでは、通常「ステップ関数」（ヘヴィサイド関数）が用いられます 。これは、入力の総和（重み付き和＋バイアス）がある基準値（バイアスを用いる場合は通常0）を超えた場合に1を、それ以外の場合に0（または-1 ）を出力します。   \n",
    "\n",
    "$$  \\begin{equation*}\n",
    "    y = \n",
    "   \\left\\{\n",
    "      \\begin{align*}\n",
    "         0 \\quad ( w_1x_1 + w_2x_2 \\leq \\theta ) \\\\\n",
    "         1 \\quad ( w_1x_1 + w_2x_2 \\gt \\theta  ) \n",
    "      \\end{align*}\n",
    "   \\right.\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### バイアスと閾値の等価性と実装上の利点\n",
    "概念的には、閾値は比較対象の値、バイアスは加算される項として区別されますが、両者はニューロンの発火しやすさを制御するという同じ目的を果たします。閾値$ \\theta $を負のバイアス $ b= − \\theta $ として扱うことで、ニューロンへの総入力（ネット入力）の計算式を、単純な内積とバイアスの和$ z = w \\cdot x + b $ で表現できます。さらに、バイアスを固定入力$ x_0 = 1 $ に対応する重み$ w_0 $​ と見なせば、$ z = w^{\\prime} \\cdot x^{}\\prime $ （ここで $ x^{\\prime}= [1,x_1​,…,x_n​]、w′ = [b,w_1​,…,w_n​]）$のように、単一の内積計算として統一的に扱えます。この表現は、特に学習アルゴリズムを実装する際に計算を簡略化し、便利です。この表記法の変遷は、数学的・実装的な利便性を追求した結果と言えるでしょう。\n",
    "\n",
    "##### 生物学的ニューロンとパーセプトロンの構成要素比較\n",
    "\n",
    "パーセプトロンの概念は生物学的ニューロンから着想を得ているため 1、両者の構成要素を対比することで、そのモデル化の意図を理解しやすくなります。以下の表は、その対応関係を示しています。\n",
    "\n",
    "| 生物学的ニューロンの要素 | 単純パーセプトロンの要素 | 説明　|\n",
    "| ----- | ---- | ---- |\n",
    "| 樹状突起 (Dendrites) | 入力 $(x_1​,…,x_n​)$ | 他のニューロンからの信号を受け取る部分 |\n",
    "| シナプス (Synapses) | 重み $(w_1​,…,w_n​)$ |  信号の伝達効率（結合強度）を調整する部分 |\n",
    "| 細胞体 (Cell Body) | 活性化関数/閾値 ($ \\theta $ または $b$) | 信号の強さが一定値を超えたか判断し、発火を決定する部分 |\n",
    "| 軸索 (Axon) | 出力 ($y$) |  処理結果の信号を次のニューロンへ伝達する部分 | \n",
    "\n",
    "この対比により、抽象的な数学モデルが、生物学的なインスピレーションに基づいていることが明確になり、初心者にとってもパーセプトロンの概念がより直感的に捉えやすくなるでしょう。\n",
    "\n",
    "### 動作メカニズム\n",
    "\n",
    "#### ネット入力の計算：重み付き和\n",
    "\n",
    "パーセプトロンの計算の第一段階は、各入力$x_i$​に対応する重み$w_i$​を掛け合わせ、それらの積を合計することです。\n",
    "次に、この重み付き和にバイアス項$b$を加えます。これにより、ネット入力$z$が計算されます。\n",
    " $$ z = \\sum_{i=1}^n ​w_i​x_i​+ b $$\n",
    "閾値$\\theta$を用いる場合は、バイアスを加えず、重み付き和\n",
    "$$ Sum = \\sum_{i=1}^n ​w_i​x_i​ $$ \n",
    "を計算します。\n",
    "\n",
    "#### 決定境界：閾値処理ロジック\n",
    "\n",
    "計算の第二段階は、計算されたネット入力$z$ または重み付き和 Sum）に対して活性化関数（ステップ関数）を適用することです 。\n",
    "バイアス$b$を用いる場合、ネット入力$z$が0より大きければ1を、そうでなければ0（または-1）を出力します。 \n",
    "閾値$\\theta$を用いる場合、重み付き和Sumが$\\theta$以上であれば1を、そうでなければ0を出力します。\n",
    "\n",
    "#### パーセプトロン出力の数式表現\n",
    "以上の計算は、ベクトル表記を用いるとより簡潔に表現できます。入力ベクトルを $x=[x_1​,…,x_n​]$、重みベクトルを$w=[w_1​,…,w_n​]$ とすると、パーセプトロンの出力 y は次のように書けます。\n",
    "$$\n",
    " y=step(w \\cdot x + b)\n",
    "$$\n",
    "ここで、step(⋅) はステップ関数を表します。\n",
    "\n",
    "![Step関数](./step_function.png)\n",
    "\n",
    "#### 幾何学的解釈：線形分離器としてのパーセプトロン\n",
    "ネット入力の計算式 $z=w \\cdot x+b$ は、線形方程式です。この式で z=0 となる条件 $w \\cdot x + b = 0$ は、入力空間 ($x$ が存在する空間) において、直線（2次元の場合）、平面（3次元の場合）、あるいは超平面（より高次元の場合）を定義します。パーセプトロンの出力は、入力点$x$がこの超平面のどちら側にあるかを示しているに過ぎません。つまり、単純パーセプトロンは本質的に、入力空間を一つの超平面で二つの領域に分割する「線形分離器」として機能します。この幾何学的な視点は、パーセプトロンがなぜ線形分離可能な問題しか解けないのかを理解する上で極めて重要です 。\n",
    "\n",
    "\n",
    "### 適切な重みの学習：パーセプトロン学習規則\n",
    "\n",
    "#### 基本的な考え方：間違いからの学習（誤り訂正）\n",
    "パーセプトロンの学習の目的は、与えられた入力 x に対して正しい出力 t （教師ラベル）を生成するように、重み w とバイアス b を見つけ出すことです 11。この学習は、ラベル付きの訓練データを用いて行われます。\n",
    "パーセプトロン学習規則は、「誤り訂正型」のアルゴリズムです。つまり、パーセプトロンが訓練データに対して誤った出力（予測 y が教師ラベル t と異なる）をした場合にのみ、重みとバイアスが調整されます 11。\n",
    "\n",
    "#### パーセプトロン学習アルゴリズム\n",
    "\n",
    "学習プロセスは、以下の手順で進められます。   \n",
    "\n",
    "1. 初期化: 重み $w_i$​ とバイアス $b$ を、小さなランダム値またはゼロで初期化します。    \n",
    "2. 訓練データの反復: 各訓練サンプル (x, 教師ラベル t) について、以下の処理を行います。    \n",
    "    a. 予測: 現在の重みとバイアスを用いて、パーセプトロンの出力$y=step(w \\cdot x+b)$ を計算します。   \n",
    "    b. 誤り判定と更新: もし予測 y が教師ラベル t と異なる場合$（y \\ne t）$、重みとバイアスを更新します。     \n",
    "3. 終了条件: 全ての訓練データに対して誤りがなくなるまで（すなわち、全ての訓練サンプルを正しく分類できるようになるまで）、ステップ2をデータセット全体に対して繰り返します（この1回の繰り返しを1エポックと呼びます）。あるいは、事前に定められた最大エポック数に達したら学習を終了します 。\n",
    "\n",
    "#### 重みとバイアスの更新規則（数式）\n",
    "予測が誤っていた場合の更新規則は、以下の式で与えられます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  w_i &​= w_i​ + \\eta (t−y) x_i​    \\\\\n",
    "  b   &= b + \\eta (t−y)  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "- $\\eta$ (イータ) は学習率（learning rate）と呼ばれる正の定数です。\n",
    "- $(t−y)$ は誤差項です。教師ラベル$t$と予測$y$が {0, 1} または {-1, +1} の場合、この値はそれぞれ ±1 または ±2 になります（実装に応じてスケーリングが必要な場合があります）。\n",
    "- $x_i$​は、更新対象の重み $w_i$​に対応する入力値です。 バイアスの更新式は、バイアスを$x_0​ = 1$ に対応する重み$w_0$​と考えれば、重みの更新式と同じ形になります。\n",
    "\n",
    "この更新規則は、パーセプトロン損失関数$L=max(0,−t \\cdot z)$ ここで$z = w \\cdot x + b、 t \\in \\{−1,1\\}$に対する勾配降下法を、誤分類された単一の点について適用したものと解釈できます。\n",
    "\n",
    "#### 学習率 ( $ \\eta $) の役割\n",
    "\n",
    "学習率$ \\eta $は、重みの更新量を制御する重要なハイパーパラメータ（学習前に人間が設定するパラメータ）です。\n",
    "- $ \\eta $が小さい場合：学習は安定しますが、収束に必要な反復回数（エポック数）が増え、学習に時間がかかる可能性があります。\n",
    "- $ \\eta $が大きい場合：学習が速く進む可能性がありますが、更新量が大きすぎるために最適解を行き過ぎてしまい、重みが発散したり、収束せずに振動したりするリスクがあります。\n",
    "適切な学習率を選択することは、学習の成功に不可欠です。\n",
    "\n",
    "#### パーセプトロン収束定理\n",
    "\n",
    "パーセプトロン学習規則には、強力な理論的保証があります。パーセプトロン収束定理によれば、もし訓練データセットが線形分離可能であるならば、パーセプトロン学習アルゴリズムは有限回の更新で、全ての訓練データを正しく分類する重みベクトル（とバイアス）を見つけることが保証されています。\n",
    "しかし、この定理が成り立つための絶対条件は、データが線形分離可能であることです。\n",
    "\n",
    "#### オンライン学習とその特性\n",
    "パーセプトロン学習規則は、訓練サンプルを一つずつ見て、誤りがあればその都度重みを更新します。これは「オンライン学習」と呼ばれる方式で、データセット全体を処理してから更新を行う「バッチ学習」（例：標準的な勾配降下法）とは対照的です 4。オンライン学習は、データの提示順序によって学習の経路が変わる可能性がありますが、線形分離可能なデータに対する収束保証は維持されます。この逐次的な更新は、メモリ効率が良いという利点がある一方で、更新が不安定になる可能性も秘めています。\n",
    "\n",
    "#### 非分離データへの対応\n",
    "収束定理の裏返しとして、もしデータが線形分離不可能な場合、パーセプトロン学習アルゴリズムは収束する保証がありません 11。重みは安定せず、無限に更新を繰り返したり、振動したりする可能性があります 11。重要なのは、線形分離不可能な場合に、単に収束しないだけでなく、「最もよくデータを分離する線（最適近似線）」のような次善の解を見つけるメカニズムも持たない点です。これは、他の線形分類器（例：ロジスティック回帰）が損失関数を最小化することで何らかの解を見つけようとするのとは対照的です。この「失敗時の挙動」が、実用上の大きな制約となります。\n",
    "\n",
    "\n",
    "### Python実装\n",
    "\n",
    "#### 論理ゲートの実装（AND, OR, NAND）\n",
    "まず、単純パーセプトロンの構造を用いて、基本的な論理ゲート（AND, OR, NAND）を実装する例を見てみましょう。ここでは、学習アルゴリズムを使わず、ゲートの真理値表を満たすように手動で設定した重みとバイアスを使用します。\n",
    "\n",
    "```Python\n",
    "def step_function(z):\n",
    "  \"\"\"ステップ関数\"\"\"\n",
    "  return 1 if z >= 0 else 0 # 閾値を0とする\n",
    "\n",
    "def AND(x1, x2):\n",
    "  \"\"\"ANDゲート（手動設定）\"\"\"\n",
    "  x = np.array([x1, x2])\n",
    "  w = np.array([0.5, 0.5]) # 重み\n",
    "  b = -0.7                 # バイアス\n",
    "  z = np.dot(w, x) + b\n",
    "  return step_function(z)\n",
    "\n",
    "def OR(x1, x2):\n",
    "  \"\"\"ORゲート（手動設定）\"\"\"\n",
    "  x = np.array([x1, x2])\n",
    "  w = np.array([0.7, 0.7]) # 重み (ANDとは異なる)\n",
    "  b = -0.3                 # バイアス (ANDとは異なる)\n",
    "  z = np.dot(w, x) + b\n",
    "  return step_function(z)\n",
    "\n",
    "def NAND(x1, x2):\n",
    "  \"\"\"NANDゲート（手動設定）\"\"\"\n",
    "  x = np.array([x1, x2])\n",
    "  w = np.array([-0.5, -0.5]) # 重み (ANDの符号反転)\n",
    "  b = 0.7                   # バイアス (ANDの符号反転)\n",
    "z = np.dot(w, x) + b\n",
    "  return step_function(z)\n",
    "\n",
    "# 動作確認\n",
    "print(\"AND(0, 0):\", AND(0, 0)) # 0\n",
    "print(\"AND(1, 1):\", AND(1, 1)) # 1\n",
    "print(\"OR(0, 0):\", OR(0, 0))   # 0\n",
    "print(\"OR(0, 1):\", OR(0, 1))   # 1\n",
    "print(\"NAND(1, 1):\", NAND(1, 1)) # 0\n",
    "print(\"NAND(0, 1):\", NAND(0, 1)) # 1\n",
    "\n",
    "```\n",
    "\n",
    "この例では、各ゲートに対して適切な重みとバイアスを設定することで、正しい論理演算が実現できることを示しています。\n",
    "\n",
    "#### データ反復（エポック）と重み更新\n",
    "\n",
    "次に、パーセプトロン学習規則を実装し、データから自動的に重みとバイアスを学習するプロセスを見ていきます。これには、訓練データを繰り返し処理するループ（エポック）と、誤りがあった場合に重みとバイアスを更新するロジックが含まれます。\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "class SimplePerceptron:\n",
    "    def __init__(self, learning_rate=0.1, n_iter=50, random_state=1):\n",
    "        \"\"\"\n",
    "        単純パーセプトロンの初期化\n",
    "\n",
    "        Parameters:\n",
    "        learning_rate (float): 学習率 (0.0より大きく1.0以下)\n",
    "        n_iter (int): 訓練データの反復回数（エポック数）\n",
    "        random_state (int): 重み初期化のための乱数シード\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.weights = None # 重みベクトル (バイアス項含む)\n",
    "        self.errors_ = None    # 各エポックでの誤分類数\n",
    "\n",
    "    def fit(self, X, t):\n",
    "        \"\"\"\n",
    "        訓練データを用いてパーセプトロンの重みを学習\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like, shape = [n_samples, n_features]): 訓練データ\n",
    "        t (array-like, shape = [n_samples]): 教師ラベル (1 or 0)\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # 重みを小さな乱数で初期化 (+1はバイアス項のため)\n",
    "        self.weights = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ =\n",
    "\n",
    "        # 教師ラベルを内部的に -1, 1 に変換 (更新式のため)\n",
    "        t_internal = np.where(t == 1, 1, -1)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, t_internal):\n",
    "                # 予測 (ネット入力計算 -> 活性化)\n",
    "                net_input = self.net_input(xi)\n",
    "                # 活性化関数 (ここでは符号関数に近い役割)\n",
    "                y = np.where(net_input >= 0.0, 1, -1)\n",
    "\n",
    "                # 誤りがあれば重みを更新\n",
    "                if y!= target:\n",
    "                    update = self.learning_rate * (target - y)\n",
    "                    self.weights[1:] += update * xi\n",
    "                    self.weights += update # バイアス項の更新 (入力x0=1とみなす)\n",
    "                    errors += 1\n",
    "            self.errors_.append(errors)\n",
    "            # 全てのサンプルが正しく分類されたら学習終了 (オプション)\n",
    "            # if errors == 0:\n",
    "            #     break\n",
    "        return self\n",
    " \n",
    "    def net_input(self, X):\n",
    "        \"\"\"ネット入力（総入力）を計算\"\"\"\n",
    "        # np.dot(X, self.weights[1:]) で重み付き和、+ self.weights でバイアス加算\n",
    "        return np.dot(X, self.weights[1:]) + self.weights\n",
    "  \n",
    "    def predict(self, X):\n",
    "        \"\"\"入力データに対するクラスラベルを予測\"\"\"\n",
    "        # ネット入力を計算し、ステップ関数（ここでは閾値0）でクラスラベルを決定\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0) # 出力は 1 or 0 に戻す\n",
    "\n",
    "# --- ANDゲートの学習と予測 ---\n",
    "if __name__ == '__main__':\n",
    "    # ANDゲートの訓練データ\n",
    "    X_train = np.array([, , , [1, 1]])\n",
    "    t_train = np.array() # ANDの出力\n",
    "\n",
    "    # パーセプトロンのインスタンス化と学習\n",
    "    ppn = SimplePerceptron(learning_rate=0.1, n_iter=10)\n",
    "    ppn.fit(X_train, t_train)\n",
    "\n",
    "    # 学習後の重みと誤分類数の推移を表示\n",
    "    print('学習後の重み:', ppn.weights)\n",
    "    # print('各エポックでの誤分類数:', ppn.errors_) # 学習の進捗確認用\n",
    "\n",
    "    # 訓練データに対する予測結果を確認\n",
    "    print('\\n訓練データに対する予測:')\n",
    "    for xi, target in zip(X_train, t_train):\n",
    "        prediction = ppn.predict(xi)\n",
    "        print(f'入力: {xi}, 予測: {prediction}, 正解: {target}')\n",
    "\n",
    "    # 新しいデータに対する予測 (例: )\n",
    "    # print('\\n新しいデータの予測:', ppn.predict(np.array()))\n",
    "```\n",
    "このコードは、パーセプトロンモデルを定義し、ANDゲートの真理値表データを使って学習させ、学習によって得られた重みを用いて予測を行う一連の流れを示しています 30。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee6674cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パーセプトロンでAND回路を作る\n",
    "# AND回路とは\n",
    "#    x1  x2  y\n",
    "#    0   0   0\n",
    "#    0   1   0\n",
    "#    1   0   0\n",
    "#    1   1   1 \n",
    "#  w1, w2, θ　は？　\n",
    "# w1 = 0.5, w2 = 0.5, θ = 0.7  0.5 * 1 + 0.5 * 1 = 1 > 0.7\n",
    "\n",
    "\"\"\"\n",
    "  OR\n",
    "  x1  x2 y\n",
    "  0   0  0\n",
    "  1   0  1\n",
    "  0   1  1  \n",
    "  1   1  1\n",
    "  \n",
    "  NAND\n",
    "  x1  x2  y\n",
    "  0   0   1\n",
    "  1   0   1\n",
    "  0   1   1\n",
    "  1   1   0 \n",
    " \n",
    " ORとNANDのw1, w2, θは？\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def AND(x1, x2):\n",
    "    w1 = 0.5\n",
    "    w2 = 0.5\n",
    "    theta = 0.7\n",
    "    y = w1 * x1 + w2 * x2\n",
    "    if y > theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def OR(x1, x2):\n",
    "    w1 = 0.5\n",
    "    w2 = 0.5\n",
    "    theta = 0.0\n",
    "    y = w1 * x1 + w2 * x2\n",
    "    if y > theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def AND_B(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w * x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def OR_B(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.0\n",
    "    tmp = np.sum(w * x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#AND_B(0, 0)\n",
    "#AND_B(0, 1)\n",
    "#AND_B(1, 0)\n",
    "#AND_B(1, 1)\n",
    "OR(1, 0)\n",
    "OR(0, 1)\n",
    "OR(1, 1)\n",
    "OR_B(0, 0)\n",
    "OR_B(1, 0)\n",
    "OR_B(0, 1)\n",
    "OR_B(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa7f29c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# パーセプトロンのインスタンス化と学習\u001b[39;00m\n\u001b[1;32m     70\u001b[0m ppn \u001b[38;5;241m=\u001b[39m SimplePerceptron(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mppn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 学習後の重みと誤分類数の推移を表示\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m学習後の重み:\u001b[39m\u001b[38;5;124m'\u001b[39m, ppn\u001b[38;5;241m.\u001b[39mweights)\n",
      "Cell \u001b[0;32mIn[31], line 42\u001b[0m, in \u001b[0;36mSimplePerceptron.fit\u001b[0;34m(self, X, t)\u001b[0m\n\u001b[1;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(net_input \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 誤りがあれば重みを更新\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m!=\u001b[39m target:\n\u001b[1;32m     43\u001b[0m     update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (target \u001b[38;5;241m-\u001b[39m y)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m xi\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "class SimplePerceptron:\n",
    "    def __init__(self, learning_rate=0.1, n_iter=50, random_state=1):\n",
    "        \"\"\"\n",
    "        単純パーセプトロンの初期化\n",
    "\n",
    "        Parameters:\n",
    "        learning_rate (float): 学習率 (0.0より大きく1.0以下)\n",
    "        n_iter (int): 訓練データの反復回数（エポック数）\n",
    "        random_state (int): 重み初期化のための乱数シード\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.weights = None # 重みベクトル (バイアス項含む)\n",
    "        self.errors_ = None    # 各エポックでの誤分類数\n",
    "\n",
    "    def fit(self, X, t):\n",
    "        \"\"\"\n",
    "        訓練データを用いてパーセプトロンの重みを学習\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like, shape = [n_samples, n_features]): 訓練データ\n",
    "        t (array-like, shape = [n_samples]): 教師ラベル (1 or 0)\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # 重みを小さな乱数で初期化 (+1はバイアス項のため)\n",
    "        self.weights = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "\n",
    "        # 教師ラベルを内部的に -1, 1 に変換 (更新式のため)\n",
    "        t_internal = np.where(t == 1, 1, -1)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, t_internal):\n",
    "                # 予測 (ネット入力計算 -> 活性化)\n",
    "                net_input = self.net_input(xi)\n",
    "                # 活性化関数 (ここでは符号関数に近い役割)\n",
    "                y = np.where(net_input >= 0.0, 1, -1)\n",
    "\n",
    "                # 誤りがあれば重みを更新\n",
    "                if y!= target:\n",
    "                    update = self.learning_rate * (target - y)\n",
    "                    self.weights[1:] += update * xi\n",
    "                    self.weights += update # バイアス項の更新 (入力x0=1とみなす)\n",
    "                    errors += 1\n",
    "            self.errors_.append(errors)\n",
    "            # 全てのサンプルが正しく分類されたら学習終了 (オプション)\n",
    "            # if errors == 0:\n",
    "            #     break\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"ネット入力（総入力）を計算\"\"\"\n",
    "        # np.dot(X, self.weights[1:]) で重み付き和、+ self.weights でバイアス加算\n",
    "        return np.dot(X, self.weights[1:]) + self.weights\n",
    "  \n",
    "    def predict(self, X):\n",
    "        \"\"\"入力データに対するクラスラベルを予測\"\"\"\n",
    "        # ネット入力を計算し、ステップ関数（ここでは閾値0）でクラスラベルを決定\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0) # 出力は 1 or 0 に戻す\n",
    "\n",
    "\n",
    "# ANDゲートの訓練データ\n",
    "X_train = np.array([[0, 0],[1, 0], [0, 1], [1, 1]])\n",
    "#t_train = np.array() # ANDの出力\n",
    "t_train = np.array([0, 0, 0, 1]) # ANDの出力\n",
    "\n",
    "# パーセプトロンのインスタンス化と学習\n",
    "ppn = SimplePerceptron(learning_rate=0.1, n_iter=10)\n",
    "ppn.fit(X_train, t_train)\n",
    "\n",
    "# 学習後の重みと誤分類数の推移を表示\n",
    "print('学習後の重み:', ppn.weights)\n",
    "# print('各エポックでの誤分類数:', ppn.errors_) # 学習の進捗確認用\n",
    "\n",
    "# 訓練データに対する予測結果を確認\n",
    "print('\\n訓練データに対する予測:')\n",
    "for xi, target in zip(X_train, t_train):\n",
    "    prediction = ppn.predict(xi)\n",
    "    print(f'入力: {xi}, 予測: {prediction}, 正解: {target}')\n",
    "\n",
    "# 新しいデータに対する予測 (例: )\n",
    "# print('\\n新しいデータの予測:', ppn.predict(np.array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c50c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"Train My Autoencoder Model\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D  \n",
    "\n",
    "random.seed(42)  # @UndefinedVariable\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\"\"\"Autoencoder Simples Model\n",
    "https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
    "参考論文 : https://arxiv.org/pdf/1812.11262.pdf\n",
    "[我々はロバストな予測のためのオートエンコーダーベースの残差ディープネットワークを提案する]\n",
    "\"\"\"\n",
    "    \n",
    "    # load mnist data\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test  = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, [-1, x_train.shape[1] * x_train.shape[2]])\n",
    "x_test  = np.reshape(x_test, [-1, x_test.shape[1] * x_test.shape[2]])\n",
    "\n",
    "# encode\n",
    "encoding_dim = 32\n",
    "input_img    = Input(shape=(x_train.shape[1], ), name = \"autoencoder\" + \"_input\")\n",
    "encoded      = Dense(encoding_dim, activation='relu')(input_img)\n",
    "encoded      = Flatten(name='flatten_e1')(encoded)\n",
    "encoded      = Dense(encoding_dim, activation='relu')(encoded)\n",
    "encoded      = Flatten(name='flatten_e2')(encoded)\n",
    "encoded      = Dense(encoding_dim, activation='relu')(encoded)\n",
    "encoded      = Flatten(name='flatten_e3')(encoded)\n",
    "encoded      = Dense(encoding_dim, activation='relu')(encoded)\n",
    "# decode\n",
    "decoded      = Dense(784, activation='sigmoid')(encoded)\n",
    "decoded      = Flatten(name='flatten_d1')(decoded)\n",
    "decoded      = Dense(784, activation='sigmoid')(decoded)\n",
    "decoded      = Flatten(name='flatten_d2')(decoded)\n",
    "decoded      = Dense(784, activation='sigmoid')(decoded)\n",
    "decoded      = Flatten(name='flatten_d3')(decoded)\n",
    "decoded      = Dense(784, activation='sigmoid')(decoded)\n",
    "    \n",
    "autoencoder  = Model(input_img, decoded)\n",
    "\n",
    "# Opt\n",
    "opt = Adam(lr=1e-4)\n",
    "#autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') <- NG\n",
    "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "    \n",
    "hist = autoencoder.fit(x_train, x_train, epochs=80, batch_size=128, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "def results_draw(x_test, decode_imgs) :\n",
    "      \n",
    "    \"\"\"Draw Autoencoder Results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "      \n",
    "    # 何個表示するか\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # オリジナルのテスト画像を表示\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # 変換された画像を表示\n",
    "        ax = plt.subplot(2, n, i+1+n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    #plt.savefig(\"autoencoder_results.png\")\n",
    "    plt.show()\n",
    "\n",
    "results_draw(x_test, decoded_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb309065",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.decorder = tf.keras.Sequential([layers.Dense(tf.math.reduce_prod(shape), activation='sigmoid'), \n",
    "                                     layers.Reshape(shape)], name = \"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a79d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# 絶対値の範囲が 有限なもの\n",
    "activations_type0 = [\n",
    "    \"sigmoid\",\n",
    "    \"tanh\",\n",
    "    \"hard_sigmoid\",\n",
    "    \"softsign\",\n",
    "]\n",
    "\n",
    "# 絶対値の範囲に上限がないもの\n",
    "activations_type1 = [\n",
    "    \"elu\",\n",
    "    \"selu\",\n",
    "    \"softplus\",\n",
    "    \"relu\",\n",
    "    \"linear\",\n",
    "]\n",
    "\n",
    "x = np.linspace(-5, 5, 101).reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for i, activations in enumerate([activations_type0, activations_type1]) :\n",
    "    ax = fig.add_subplot(2, 1, i + 1)\n",
    "    for activation_str in activations:\n",
    "        model = Sequential()\n",
    "        model.add(Activation(activation_str, input_shape=(1,)))\n",
    "        y = model.predict(x).ravel()\n",
    "        ax.plot(x, y, label=activation_str)\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a16ec",
   "metadata": {},
   "source": [
    "## 再急降下法\n",
    "\n",
    "多くの場合、学習の問題は、与えられた評価関数を最適とするようなパラメータを求める問題として定式\n",
    "化されます。従って、学習のためには、その最適化問題を解くための手法が必要になります。最適化手法に\n",
    "は、簡単なものから高速性や安定性のために工夫した複雑手法まで、多くの手法がありますが、ここでは、\n",
    "最も簡単な最適化手法のひとつである最急降下法と呼ばれる最適化手法の基本的な考え方について理解し\n",
    "そのプログラムを作ってみることにします\n",
    "\n",
    "### 問題\n",
    "あるパラメータa の良さの評価尺度が以下のような２次の関数  \n",
    "\n",
    " $$ f(a) = (a - 1.0) ^ 2 $$\n",
    "\n",
    " で与えられたとします。このとき、この評価関数が最小となるパラメータ a の(最適解) を求めなさい。  \n",
    "\n",
    " $$ 微分 :     \\frac{\\partial f}{\\partial a} = 2(a - 1.0) $$\n",
    "\n",
    "### 再急降下法\n",
    "最急降下法 最急降下法は、ある適当な初期値(初期パラメータ) からはじめて、その値を繰り返し更新する\n",
    "(修正する) ことにより、最適なパラメータの値を求める方法(繰り返し最適化手法) の最も基本的で\n",
    "単な方法です。\n",
    "\n",
    "$$ 更新式 :  \\alpha^{(k + 1)} = \\alpha^{(k)} - 2\\alpha(a - 1.0) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sympy as sy\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# SymPy Plotting Backends (SPB)\n",
    "#from spb import plot, plot_implicit\n",
    "from spb import plot\n",
    "\n",
    "# グラフを SVG で Notebook にインライン表示\n",
    "# これは試し\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "# おまじない\n",
    "sy.init_printing()\n",
    "\n",
    "# π，ネイピア数，虚数単位\n",
    "from sympy import pi, E, I\n",
    "\n",
    "\n",
    "a = sy.symbols('a')\n",
    "\n",
    "func = lambda x: (x -1.0) ** 2\n",
    "display(Math(r\" func = %s\" % sy.latex(func(a))))\n",
    "\n",
    "dfunc = sy.diff(func(a), a)\n",
    "display(Math(r\" \\frac{d}{da} func = %s\" % sy.latex(dfunc)))\n",
    "\n",
    "print(\"value = {}\".format(sy.solve(dfunc)))\n",
    "\n",
    "plot(func(a), (a, -2, 4), line_color='r')\n",
    "\n",
    "def main() :\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    alpha = 0.1\n",
    "    rng = np.random.default_rng()\n",
    "    _a = 100 * (rng.random() - 0.5)\n",
    "    \n",
    "    for _ in range(100) :\n",
    "        _a = _a - alpha * dfunc.subs(a, _a)\n",
    "        \n",
    "    return _a\n",
    "\n",
    "\n",
    "print(\" result = \", main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"autoencoderを実装する\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# 学習データを用意する\n",
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
    "\n",
    "# データを正規化する\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test  = x_test.astype('float32') / 255.\n",
    "\n",
    "print(\"x_train.shape = \", x_train.shape)\n",
    "print(\"x_test.shape  = \", x_test.shape)\n",
    "\n",
    "# 潜在空間の次元数\n",
    "latent_dim   = 64\n",
    "# targetの次元数\n",
    "target_shape = x_train.shape[1:]\n",
    "\n",
    "# batch size, epoch\n",
    "batch_size = 128\n",
    "epochs     = 10\n",
    "\n",
    "# modelの定義\n",
    "model = Sequential(name = \"autoencoder\")\n",
    "encoder_1 = Flatten(input_shape = target_shape, name = \"encoder_1\")\n",
    "encoder_2 = Dense(latent_dim, activation = 'relu', name = \"encoder_2\")\n",
    "model.add(encoder_1)\n",
    "model.add(encoder_2)\n",
    "#decorder_1 = Dense(tf.math.reduce_prod(target_shape), activation = 'sigmoid', name = \"decoder_1\")\n",
    "decorder_1 = Dense(target_shape[0] * target_shape[1], activation = 'sigmoid', name = \"decoder_1\")\n",
    "decorder_2 = Reshape(target_shape, name = \"decoder_2\")\n",
    "model.add(decorder_1)\n",
    "model.add(decorder_2)\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer = optimizers.Adam(), loss = losses.MeanSquaredError())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 学習\n",
    "history = model.fit(x_train, x_train, \n",
    "                    epochs  = epochs, batch_size = batch_size, \n",
    "                    shuffle = True,  validation_data = (x_test, x_test))\n",
    "\n",
    "# 結果を評価する\n",
    "decoded_imgs = model.predict(x_test)\n",
    "print(\"decoded_imgs.shape = \", decoded_imgs.shape)\n",
    "\n",
    "def results_draw(x_test, decode_imgs) :\n",
    "      \n",
    "    \"\"\"Draw Autoencoder Results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "      \n",
    "    # 何個表示するか\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # オリジナルのテスト画像を表示\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # 変換された画像を表示\n",
    "        ax = plt.subplot(2, n, i+1+n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results_draw(x_test, decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e566203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"CNN autoencoderを実装する\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, Input\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# 学習データを用意する\n",
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
    "\n",
    "# データを正規化する\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test  = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test  = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "print(\"x_train.shape = \", x_train.shape)\n",
    "print(\"x_test.shape  = \", x_test.shape)\n",
    "\n",
    "# targetの次元数\n",
    "target_shape = x_train.shape[1:]\n",
    "print(\"target_shape = \", target_shape)\n",
    "\n",
    "# batch size, epoch\n",
    "batch_size = 128\n",
    "epochs     = 10\n",
    "\n",
    "# modelの定義\n",
    "model = Sequential(name = \"cnn_autoencoder\")\n",
    "encoder_1 = Input(shape = target_shape, name = \"encoder_1\")\n",
    "encoder_2 = Conv2D(32, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2, name = \"encoder_2\")\n",
    "encoded_3 = Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2, name = \"encoder_3\")\n",
    "model.add(encoder_1)\n",
    "model.add(encoder_2)\n",
    "model.add(encoded_3)\n",
    "decorder_1 = Conv2DTranspose(64, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2, name = \"decoder_1\")\n",
    "decorder_2 = Conv2DTranspose(32, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2, name = \"decoder_2\")\n",
    "decorder_3 = Conv2D(1, kernel_size = (3, 3), activation = 'sigmoid', padding = 'same', name = \"decoder_3\")\n",
    "model.add(decorder_1)\n",
    "model.add(decorder_2)\n",
    "model.add(decorder_3)\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer = optimizers.Adam(), loss = losses.BinaryCrossentropy())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 学習\n",
    "history = model.fit(x_train, x_train, \n",
    "                    epochs  = epochs, batch_size = batch_size, \n",
    "                    shuffle = True,  validation_data = (x_test, x_test))\n",
    "\n",
    "# 結果を評価する\n",
    "decoded_imgs = model.predict(x_test)\n",
    "print(\"decoded_imgs.shape = \", decoded_imgs.shape)\n",
    "\n",
    "def results_draw(x_test, decode_imgs) :\n",
    "      \n",
    "    \"\"\"Draw Autoencoder Results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "      \n",
    "    # 何個表示するか\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # オリジナルのテスト画像を表示\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # 変換された画像を表示\n",
    "        ax = plt.subplot(2, n, i+1+n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results_draw(x_test, decoded_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
