{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17098d",
   "metadata": {},
   "source": [
    "## Spam mail分類を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07432d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nisida/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nisida/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "-------------\n",
      "hum count   =  4825\n",
      "supam count =  747\n",
      "-------------\n",
      "max_token_len =  171\n",
      "[665, 202, 221, 666, 667, 668, 595, 477, 21, 669, 294, 670, 553]\n",
      "id_label # =  1\n",
      "df[LABEL].values 100 =  ham\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\"\"\" span.cvsを読み込み \"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#from typing import List\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "FILE_PATH = './spam_data/spam.csv'\n",
    "LABEL     = 'label'\n",
    "MESSAGE   = 'message'\n",
    "\n",
    "print(pd.__version__)\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, encoding='latin-1', header=None, names = [LABEL, MESSAGE, 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "\n",
    "print(\"-------------\")\n",
    "print(\"hum count   = \", sum([ x == \"ham\" for x in df[LABEL].values]))\n",
    "print(\"supam count = \", sum([ x == \"spam\" for x in df[LABEL].values]))\n",
    "print(\"-------------\")\n",
    "\n",
    "# ノイズを取り除く\n",
    "clean_str = [x.replace('?', '') for x in df[MESSAGE]]\n",
    "clean_str = [x.replace('!', '') for x in clean_str]\n",
    "clean_str = [x.replace(',', '') for x in clean_str]\n",
    "clean_str = [x.replace('.', '') for x in clean_str]\n",
    "lower_msg = [x.lower() for x in clean_str] \n",
    "\n",
    "#all_token = [nltk.tokenize.word_tokenize(x) for x in lower_msg]\n",
    "#print(all_token[100])\n",
    "# windowsの場合には、以下のようにする\n",
    "# 文書のtokenを作成する\n",
    "all_tokens = [x.split() for x in lower_msg]\n",
    "max_token_len = max([len(x) for x in all_tokens])\n",
    "print(\"max_token_len = \", max_token_len)\n",
    "# stop wordsを取り除く\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = [[x for x in words if x not in stopwords] for words in all_tokens]\n",
    "#print(all_tokens[100])\n",
    "#print(\"keys # = \", len(all_tokens))\n",
    "\n",
    "# all_tokensのコーパスを作成する\n",
    "cop_dict = {'nishida': 0}\n",
    "for token in all_tokens:\n",
    "    for word in token:\n",
    "        if word not in cop_dict:\n",
    "            cop_dict[word] = max(cop_dict.values()) + 1            \n",
    "#print(\"cop_dict # = \", cop_dict)\n",
    "\n",
    "# all_tokensの中の単語を、cop_dictの単語IDに置き換える\n",
    "all_token_ids = []\n",
    "for token in all_tokens:\n",
    "    token_ids = []\n",
    "    for word in token:\n",
    "        token_ids.append(cop_dict[word])\n",
    "    all_token_ids.append(token_ids)\n",
    "print(all_token_ids[100])\n",
    "\n",
    "# hamとspamのラベルを数値に置き換える hum : 1 , spam : 2\n",
    "id_label = []\n",
    "for label in df[LABEL]:\n",
    "    if label == \"ham\":\n",
    "        id_label.append(1)\n",
    "    else:\n",
    "        id_label.append(2)\n",
    "\n",
    "#print(\"id_label # = \", id_label[100])\n",
    "#print(\"df[LABEL].values 100 = \", df[LABEL].values[100])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf3861",
   "metadata": {},
   "source": [
    "{'name': 1, 'price':2} ->  all_token内の全ての単語にユニークなID（数字)をふる（Pythonの辞書で作成する）. -> コーパスと呼ぶ。\n",
    "\n",
    "気になるpoint\n",
    "\n",
    "token1 = [1,3, 6, 7] , token2 = [10, 13, 14, 15 16, 17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "print(nltk.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
