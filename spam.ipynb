{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17098d",
   "metadata": {},
   "source": [
    "## Spam mail分類を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2ae6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nisida/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nisida/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "-------------\n",
      "hum count   =  4825\n",
      "supam count =  747\n",
      "-------------\n",
      "max_token_len =  171\n",
      "[665, 202, 221, 666, 667, 668, 595, 477, 21, 669, 294, 670, 553]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import libs.load_spam_data\n",
    "import importlib\n",
    "importlib.reload(libs.load_spam_data)\n",
    "\n",
    "FILE_PATH = './spam_data/spam.csv'\n",
    "train_tokes, train_labels, test_tokes, test_labels = libs.load_spam_data.load_spamd_data(FILE_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf3861",
   "metadata": {},
   "source": [
    "{'name': 1, 'price':2} ->  all_token内の全ての単語にユニークなID（数字)をふる（Pythonの辞書で作成する）. -> コーパスと呼ぶ。\n",
    "\n",
    "気になるpoint\n",
    "\n",
    "token1 = [1,3, 6, 7] , token2 = [10, 13, 14, 15 16, 17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59367fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# データセットの準備\n",
    "# ここでは、簡潔のためダミーデータを使用します。\n",
    "# 実際には、スパムメール分類の公開データセットを使用してください。\n",
    "# 例: KaggleのSMS Spam Collection datasetなど\n",
    "data = {'text': ['Call me back urgent!!', 'Hello, how are you?', 'WIN a prize NOW!', 'Meeting at 3 PM.', 'Free money offer!', 'Please disregard.'],\n",
    "        'label': ['spam', 'ham', 'spam', 'ham', 'spam', 'ham']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ラベルのエンコーディング\n",
    "# 'spam' を 1 に、'ham' を 0 に変換\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])\n",
    "\n",
    "# データの分割\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# テキストの前処理 (トークン化とパディング)\n",
    "\n",
    "# 語彙サイズの設定\n",
    "vocab_size = 1000\n",
    "# 各シーケンスの最大長\n",
    "max_len = 50\n",
    "\n",
    "# Tokenizerの初期化と訓練データでのfitting\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# テキストをシーケンスに変換\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# シーケンスのパディング\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# NumPy配列に変換 (TensorFlow/Kerasでの使用のため)\n",
    "train_padded = np.array(train_padded)\n",
    "test_padded = np.array(test_padded)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# モデルの構築\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 16, input_length=max_len),\n",
    "    keras.layers.GlobalAveragePooling1D(), # または Flatten(), LSTM(), Conv1D() など\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid') # スパムかハムかの二値分類のためsigmoid\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# モデルのサマリーを表示\n",
    "model.summary()\n",
    "\n",
    "# モデルの学習\n",
    "num_epochs = 30\n",
    "history = model.fit(train_padded, y_train, epochs=num_epochs, validation_data=(test_padded, y_test), verbose=2)\n",
    "\n",
    "# モデルの評価\n",
    "loss, accuracy = model.evaluate(test_padded, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# 新しいメールでの予測例\n",
    "sample_email = [\"Claim your free gift now!\", \"Meeting moved to tomorrow.\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_email)\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "predictions = model.predict(sample_padded)\n",
    "\n",
    "# 予測結果の表示\n",
    "print(\"\\n--- 予測結果 ---\")\n",
    "for i, email in enumerate(sample_email):\n",
    "    # シグモイド出力が0.5より大きければスパムと判断\n",
    "    predicted_label = \"spam\" if predictions[i] > 0.5 else \"ham\"\n",
    "    print(f\"メール: \\\"{email}\\\" -> 予測: {predicted_label} ({predictions[i][0]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
